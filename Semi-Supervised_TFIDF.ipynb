{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use TFIDF and Cosine similarity to define high probability scores and low probability scores to define positive and negative matches.  Then use semi-supervised learning to fill in the labels for the remaining matches.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.metrics import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "% matplotlib inline\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "stop = stopwords.words('english')\n",
    "dir = 'C:\\\\Users\\\\Travis Park\\\\Google Drive\\\\UN\\\\Team MOZART RAP\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tfidfvectorizer =TfidfVectorizer(ngram_range = (1,2),sublinear_tf=False, \n",
    "                                 min_df = 0.01, max_features = 1100) \n",
    "def TFIDFtotable(inputlist1, inputlist2, newcollist):\n",
    "    corpus = inputlist1 + inputlist2\n",
    "    \n",
    "    x = tfidfvectorizer.fit_transform(corpus)\n",
    "    x = pd.DataFrame((x * x.T).A)\n",
    "    \n",
    "    x = x.ix[:len(inputlist1)-1,len(inputlist1):]\n",
    "    x.columns = newcollist\n",
    "    x['Text'] = trimtext\n",
    "    x = pd.merge(x, Paragraph_DF, how = 'left', left_on = 'Text', right_on = 'StemText')\n",
    "    x=x.drop(['Text_x','Text_y','TrimText','StemText'],1)\n",
    "    x=pd.melt(x, id_vars=['Number'])\n",
    "    x.rename(columns={'Number':'Paragraph','variable': 'Target','value':'T_Cosine'}, inplace = True)\n",
    "    TFIDFTable = x    \n",
    "   \n",
    "    return TFIDFTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def RemoveVerbs(string):\n",
    "    if len(string) > 0:\n",
    "        x = nltk.pos_tag(string.split(' '))\n",
    "        x = [(word, tag) for (word, tag) in x\n",
    "             if tag.startswith('N') \n",
    "             or tag.startswith('J')\n",
    "             or tag.startswith('R')]\n",
    "        x = ' '.join([i[0] for i in x])\n",
    "        return x\n",
    "    else:\n",
    "        return 'x x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Goal</th>\n",
       "      <th>OrigionalText</th>\n",
       "      <th>GoalText</th>\n",
       "      <th>TrimGoal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Poverty</td>\n",
       "      <td>Poverty End poverty in all its forms everywhere</td>\n",
       "      <td>poverty forms eveywhere</td>\n",
       "      <td>poverty forms eveywhere</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID     Goal                                    OrigionalText  \\\n",
       "0   1  Poverty  Poverty End poverty in all its forms everywhere   \n",
       "\n",
       "                  GoalText                 TrimGoal  \n",
       "0  poverty forms eveywhere  poverty forms eveywhere  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SDG_Targets = pd.read_excel(dir +'SDG_Goals.xlsx', names = ['ID', 'Goal','OrigionalText','GoalText'],sheetname = 'Sheet1')\n",
    "SDG_Targets['TrimGoal'] = ''\n",
    "\n",
    "targtext = []\n",
    "x=0\n",
    "\n",
    "while x < len(SDG_Targets):\n",
    "    y = SDG_Targets['GoalText'][x].lower().replace('-',' ').split(' ')\n",
    "    y = [z for z in y if z.isalpha() if z not in stop]\n",
    "    targtext.append(' '.join(y))\n",
    "    x=x+1\n",
    "    \n",
    "SDG_Targets['TrimGoal'] = targtext\n",
    "SDG_Targets['ID'] = SDG_Targets['ID'].astype(int)\n",
    "SDG_Targets.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = list(SDG_Targets['TrimGoal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = open(dir + 'Bhutan_Doc_test.txt', 'r')\n",
    "f = f.read().replace('. \\n\\n','.\\n\\n').replace('-',' ').replace('\\x0c',' ').replace('...','').replace('Eleventh Five Year Plan',' ').split('.\\n\\n')\n",
    "f = [w.replace('\\n',' ') for w in f]\n",
    "\n",
    "w=0\n",
    "wlist=[]\n",
    "while w<len(f):\n",
    "    wlist.append(w)\n",
    "    w=w+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Paragraph_DF = pd.DataFrame(columns = ['Number','Text', 'TrimText'])\n",
    "Paragraph_DF['Number'] = wlist\n",
    "Paragraph_DF['Text'] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ttext = []\n",
    "x = 0 \n",
    "while x < len(Paragraph_DF):\n",
    "    y = Paragraph_DF['Text'][x].lower().split(' ')\n",
    "    y = [z for z in y if z.isalpha() if z not in stop]\n",
    "    y = RemoveVerbs(' '.join(y))\n",
    "    ttext.append(y)\n",
    "    x=x+1\n",
    "    \n",
    "Paragraph_DF['TrimText'] = ttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stemtext = []\n",
    "x = 0\n",
    "while x<len(Paragraph_DF):\n",
    "    y = Paragraph_DF['TrimText'][x].split(' ')\n",
    "    y = ' '.join([porter.stem(z) for z in y])\n",
    "    stemtext.append(y)\n",
    "    x=x+1\n",
    "    \n",
    "Paragraph_DF['StemText'] = stemtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Text</th>\n",
       "      <th>TrimText</th>\n",
       "      <th>StemText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Main Document Volume I  1   ELEVENTH FIVE ...</td>\n",
       "      <td>main document volume eleventh year plan june m...</td>\n",
       "      <td>main document volum eleventh year plan june ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number                                               Text  \\\n",
       "0       0      Main Document Volume I  1   ELEVENTH FIVE ...   \n",
       "\n",
       "                                            TrimText  \\\n",
       "0  main document volume eleventh year plan june m...   \n",
       "\n",
       "                                            StemText  \n",
       "0  main document volum eleventh year plan june ma...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Paragraph_DF.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alldocs = ttext+targtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07972566180440664"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stuff = \"ngram_range = (1,2),sublinear_tf=False, min_df = 0.01, max_features = 1100\"\n",
    "tfidfvectorizer =TfidfVectorizer(ngram_range = (1,2),sublinear_tf=False, \n",
    "                                 min_df = 0.01, max_features = 1100) \n",
    "\n",
    "x = tfidfvectorizer.fit_transform(alldocs)\n",
    "y = pd.DataFrame((x * x.T).A)\n",
    "\n",
    "z = y.ix[:len(ttext)-1,len(ttext):]\n",
    "z.columns = list(SDG_Targets['ID'])\n",
    "\n",
    "z1=z.unstack().reset_index()\n",
    "z1.columns = ['Goal','Paragraph','CosineSimilarity']\n",
    "z2 = z1[(z1['Goal'] != z1['Paragraph']) & (z1['CosineSimilarity'] > 0)]\n",
    "\n",
    "z2.sort_values(by = 'CosineSimilarity', ascending = False).head()\n",
    "\n",
    "avg = z2['CosineSimilarity'].mean()\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zx = z.reset_index()\n",
    "zx.rename(columns={'index': 'paragraph'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1 = pd.DataFrame(x.toarray(),columns = [tfidfvectorizer.get_feature_names()])\n",
    "x1 = x1.ix[:len(ttext)-1,]\n",
    "x1 = x1.reset_index()\n",
    "c = zx.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1.rename(columns={'level_0': 'paragraph'}, inplace=True)\n",
    "xx = pd.merge(x1, zx, how = 'left', left_on = 'paragraph', right_on = 'paragraph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     0.085441\n",
      "2     0.095492\n",
      "3     0.132588\n",
      "4     0.110067\n",
      "5     0.000000\n",
      "6     0.103680\n",
      "7     0.075807\n",
      "8     0.107786\n",
      "9     0.091713\n",
      "10    0.110070\n",
      "11    0.085386\n",
      "12    0.087348\n",
      "13    0.000000\n",
      "14    0.133220\n",
      "15    0.065464\n",
      "16    0.102563\n",
      "17    0.091643\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "q = xx.ix[:,len(xx.columns)-17:].quantile(q = .95)\n",
    "print(q)\n",
    "q = list(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < len(q):\n",
    "    j = 0\n",
    "    while j < len(zx):\n",
    "        #print(zx.ix[j,i+1])\n",
    "        if zx.ix[j,i+1] >= q[i]:\n",
    "            zx.ix[j,i+1] = 1\n",
    "        elif zx.ix[j,i+1] < 0.0000001:\n",
    "            zx.ix[j,i+1] = 0\n",
    "        else:\n",
    "            zx.ix[j,i+1] = -1\n",
    "        j=j+1\n",
    "    i=i+1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = x1\n",
    "y = zx.ix[:,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-1.0, 0.0, 1.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0, 1.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = LabelSpreading(kernel='rbf', gamma=20, n_neighbors=20,\n",
    "                    alpha=0.2, max_iter=50, tol=0.001, n_jobs=-1).fit(X, y)\n",
    "#ls100 = (label_propagation.LabelSpreading().fit(X, y), y)\n",
    "#rbf_svc = (svm.SVC(kernel='rbf').fit(X, y), y)\n",
    "ls.score(X,y)\n",
    "a = ls.score(X,y)\n",
    "output_labels = ls.transduction_\n",
    "set(output_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1['Goal6'] = list(output_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paragraph    338.0\n",
       "Goal6          1.0\n",
       "Name: 338, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1[['paragraph','Goal6']].ix[338,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, output_labels, test_size=0.50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.94075403949730696, 0.94086021505376349, -0.00010617555645653365)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Neural Net\n",
    "nn = MLPClassifier(solver='adam', activation = 'logistic',\n",
    "                   hidden_layer_sizes=(50, 10), random_state=1)\n",
    "\n",
    "nn_pred = nn.fit(X_train,y_train).predict(X_test)\n",
    "nn.score(X_train,y_train), nn.score(X_test,y_test), nn.score(X_train,y_train)-nn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[525,   0],\n",
       "       [ 33,   0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrx = confusion_matrix(y_test,nn_pred)\n",
    "mtrx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0, 0.0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gauss = GaussianNB()\n",
    "gauss_pred = gauss.fit(X_train, y_train).predict(X_test)\n",
    "gauss.score(X_train,y_train), gauss.score(X_test,y_test), gauss.score(X_train,y_train)-gauss.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[525,   0],\n",
       "       [  0,  33]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrx = confusion_matrix(y_test,gauss_pred)\n",
    "mtrx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.94075403949730696, 0.94086021505376349, -0.00010617555645653365)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###LogReg\n",
    "lr = LogisticRegression(penalty = 'l2', dual = True)\n",
    "lr_pred = lr.fit(X_train, y_train).predict(X_test)\n",
    "lr.score(X_train,y_train), lr.score(X_test, y_test),lr.score(X_train,y_train)-lr.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[525,   0],\n",
       "       [ 33,   0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrx = confusion_matrix(y_test,lr_pred)\n",
    "mtrx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[525,   0],\n",
       "       [  0,  33]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Adaptive Boosting\n",
    "ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5),n_estimators=1,learning_rate=1)\n",
    "\n",
    "ada_pred = ada.fit(X_train,y_train).predict(X_test)\n",
    "ada.score(X_train,y_train), ada.score(X_test,y_test), ada.score(X_train,y_train)-ada.score(X_test,y_test)\n",
    "\n",
    "mtrx = confusion_matrix(y_test,ada_pred)\n",
    "mtrx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1115, 973)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.30000000000001\n",
      "gauss - 1.0 1.0 0.0\n",
      "[[422   0]\n",
      " [  0  24]]\n",
      "ada - 1.0 1.0 0.0\n",
      "[[422   0]\n",
      " [  0  24]]\n",
      "perc - 0.937219730942 0.946188340807 -0.00896860986547\n",
      "[[422   0]\n",
      " [ 24   0]]\n",
      "knn - 1.0 0.964125560538 0.0358744394619\n",
      "[[413   9]\n",
      " [  7  17]]\n",
      "bnb - 0.925261584454 0.919282511211 0.00597907324365\n",
      "[[397  25]\n",
      " [ 11  13]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Travis Park\\Anaconda3_1\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:489: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  mask[np.argsort(scores, kind=\"mergesort\")[-self.k:]] = 1\n"
     ]
    }
   ],
   "source": [
    "j = .1\n",
    "k1 = len(X.columns)*(.1)\n",
    "print(k1)\n",
    "X_new = SelectKBest(chi2, k=k1).fit_transform(X, y)\n",
    "X_new.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, output_labels, test_size=0.4, random_state=42)\n",
    "\n",
    "gauss = GaussianNB()\n",
    "gauss_pred = gauss.fit(X_train, y_train).predict(X_test)\n",
    "print('gauss -',gauss.score(X_train,y_train), gauss.score(X_test,y_test), gauss.score(X_train,y_train)-gauss.score(X_test,y_test))\n",
    "print(confusion_matrix(y_test,gauss_pred))\n",
    "\n",
    "###Adaptive Boosting\n",
    "ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=4),n_estimators=1,learning_rate=1)\n",
    "ada_pred = ada.fit(X_train,y_train).predict(X_test)\n",
    "print('ada -',ada.score(X_train,y_train), ada.score(X_test,y_test), ada.score(X_train,y_train)-ada.score(X_test,y_test))\n",
    "print(confusion_matrix(y_test,ada_pred))\n",
    "\n",
    "perc = Perceptron(alpha = 1, penalty = None,fit_intercept = False)\n",
    "perc_pred = perc.fit(X_train, y_train).predict(X_test)\n",
    "print('perc -', perc.score(X_train,y_train), perc.score(X_test,y_test), perc.score(X_train,y_train)-perc.score(X_test,y_test))\n",
    "print(confusion_matrix(y_test,perc_pred))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 2, algorithm = 'auto', weights = 'distance')\n",
    "knn_pred = knn.fit(X_train, y_train).predict(X_test)\n",
    "print('knn -',knn.score(X_train,y_train), knn.score(X_test,y_test), knn.score(X_train,y_train)-knn.score(X_test,y_test))\n",
    "print(confusion_matrix(y_test,knn_pred))\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB(alpha = 4)\n",
    "bnb_pred = bnb.fit(X_train, y_train).predict(X_test)\n",
    "print('bnb -',bnb.score(X_train,y_train), bnb.score(X_test,y_test), bnb.score(X_train,y_train)-bnb.score(X_test,y_test))\n",
    "print(confusion_matrix(y_test,bnb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88490284006 0.881165919283 0.00373692077728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[393,  29],\n",
       "       [ 24,   0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###SVC\n",
    "svc = SVC(kernel = 'sigmoid',C=10, gamma='auto', probability = True)\n",
    "svc_pred = svc.fit(X_train, y_train).predict(X_test)\n",
    "print(svc.score(X_train,y_train), svc.score(X_test, y_test),svc.score(X_train,y_train)-svc.score(X_test,y_test))\n",
    "confusion_matrix(y_test,svc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-76c3ffed3cce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"darkgrid\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor_codes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswarmplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Goal\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"CosineSimilarity\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mz4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medgecolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpalette\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Set2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'z4' is not defined"
     ]
    }
   ],
   "source": [
    "sns.set(style=\"darkgrid\", color_codes=True)\n",
    "plt = sns.swarmplot(x=\"Goal\", y=\"CosineSimilarity\", data=z4, size = 5, edgecolor = 'w', palette = 'Set2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
